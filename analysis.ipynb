{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssGOxj5Ldhxa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "def load_results(base_dir):\n",
        "    results = {\n",
        "        'gnn_baseline': [],\n",
        "        'gat_ablation': [],\n",
        "        'dagnn_main': []\n",
        "    }\n",
        "\n",
        "    json_files = glob.glob(f\"{base_dir}/*.json\")\n",
        "    for filepath in json_files:\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            model_name = data['model_name']\n",
        "            if model_name in results:\n",
        "                results[model_name].append(data)\n",
        "\n",
        "    return results\n",
        "\n",
        "def calculate_statistics(results):\n",
        "    stats_dict = {}\n",
        "\n",
        "    for model_type, runs in results.items():\n",
        "        if not runs:\n",
        "            continue\n",
        "\n",
        "        metrics = {\n",
        "            'f1_macro': [run['f1_macro'] for run in runs],\n",
        "            'accuracy': [run['accuracy'] for run in runs]\n",
        "        }\n",
        "\n",
        "        stats_dict[model_type] = {\n",
        "            metric: {\n",
        "                'mean': np.mean(values),\n",
        "                'std_err': stats.sem(values),\n",
        "                'values': values\n",
        "            }\n",
        "            for metric, values in metrics.items()\n",
        "        }\n",
        "\n",
        "    return stats_dict\n",
        "\n",
        "def perform_significance_tests(stats_dict):\n",
        "    metrics = ['f1_macro', 'accuracy']\n",
        "    matchups = [\n",
        "        ('dagnn_main', 'gnn_baseline'),\n",
        "        ('dagnn_main', 'gat_ablation'),\n",
        "        ('gat_ablation', 'gnn_baseline')\n",
        "    ]\n",
        "\n",
        "    significance_results = []\n",
        "\n",
        "    for metric in metrics:\n",
        "        print(f\"\\nTesting metric: {metric}\")\n",
        "        for model1, model2 in matchups:\n",
        "            print(f\"Comparing {model1} vs {model2}\")\n",
        "\n",
        "            if model1 in stats_dict and model2 in stats_dict:\n",
        "                values1 = stats_dict[model1][metric]['values']\n",
        "                values2 = stats_dict[model2][metric]['values']\n",
        "\n",
        "                if len(values1) > 0 and len(values2) > 0:\n",
        "                    t_stat, p_value = stats.ttest_ind(values1, values2)\n",
        "\n",
        "                    significance_results.append({\n",
        "                        'metric': metric,\n",
        "                        'model1': model1,\n",
        "                        'model2': model2,\n",
        "                        'p_value': p_value\n",
        "                    })\n",
        "\n",
        "    df = pd.DataFrame(significance_results)\n",
        "    return df\n",
        "\n",
        "def create_comparison_table(stats_dict):\n",
        "    rows = []\n",
        "    metrics = ['f1_macro', 'accuracy']\n",
        "\n",
        "    for model in stats_dict.keys():\n",
        "        row = {'Model': model}\n",
        "        for metric in metrics:\n",
        "            mean = stats_dict[model][metric]['mean']\n",
        "            se = stats_dict[model][metric]['std_err']\n",
        "            row[metric] = f\"{mean:.4f} Â± {se:.4f}\"\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def plot_comparison(stats_dict, save_dir=None):\n",
        "    metrics = ['f1_macro', 'accuracy']\n",
        "    models = list(stats_dict.keys())\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(metrics), figsize=(10, 5))\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        data = []\n",
        "        labels = []\n",
        "\n",
        "        for model in models:\n",
        "            values = stats_dict[model][metric]['values']\n",
        "            data.extend(values)\n",
        "            labels.extend([model] * len(values))\n",
        "\n",
        "        sns.boxplot(x=labels, y=data, ax=axes[i])\n",
        "        axes[i].set_title(metric)\n",
        "        axes[i].set_xlabel('Model')\n",
        "        axes[i].set_ylabel('Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_dir:\n",
        "        plt.savefig(f\"{save_dir}/model_comparison.png\")\n",
        "    plt.show()\n",
        "\n",
        "def run_statistical_analysis(base_dir, save_dir=None):\n",
        "\n",
        "    if save_dir is None:\n",
        "        save_dir = base_dir\n",
        "\n",
        "    results = load_results(base_dir)\n",
        "    stats_dict = calculate_statistics(results)\n",
        "    comparison_table = create_comparison_table(stats_dict)\n",
        "    significance_tests = perform_significance_tests(stats_dict)\n",
        "    plot_comparison(stats_dict, save_dir)\n",
        "\n",
        "    if save_dir:\n",
        "        comparison_table.to_csv(f\"{save_dir}/model_comparison.csv\")\n",
        "        significance_tests.to_csv(f\"{save_dir}/significance_tests.csv\")\n",
        "\n",
        "    return {\n",
        "        'comparison': comparison_table,\n",
        "        'significance_tests': significance_tests,\n",
        "        'detailed_stats': stats_dict\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content'\n",
        "analysis_results = run_statistical_analysis(base_dir)"
      ],
      "metadata": {
        "id": "dvuTTNLqdlFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}