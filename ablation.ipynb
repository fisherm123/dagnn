{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb>=1.3.3 torch_geometric pyvis torch torch-scatter\n"
      ],
      "metadata": {
        "id": "mYUcgzmlYPY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_scatter import scatter_add, scatter_softmax\n",
        "import numpy as np\n",
        "from torch_geometric.data import DataLoader\n",
        "from ogb.graphproppred import PygGraphPropPredDataset\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import time\n",
        "from datetime import timedelta, datetime\n",
        "from torch.amp import GradScaler, autocast\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "ppWge21jD6kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_list, labels, mask_indices):\n",
        "        self.data_list = []\n",
        "        self.labels = []\n",
        "        self.mask_indices = []\n",
        "\n",
        "        for data, label, mask_idx in zip(data_list, labels, mask_indices):\n",
        "            if mask_idx < data.num_nodes:\n",
        "                self.data_list.append(data)\n",
        "                self.labels.append(label)\n",
        "                self.mask_indices.append(mask_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data_list[idx]\n",
        "        data.x = data.x.type(torch.float)\n",
        "        data.y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        data.mask_index = torch.tensor(self.mask_indices[idx], dtype=torch.long)\n",
        "        return data"
      ],
      "metadata": {
        "id": "oXMA1wMA-W_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_node_token(data, mask_token_id=-1):\n",
        "    node_count = data.x.size(0)\n",
        "\n",
        "    mask_idx = random.randint(0, node_count - 1)\n",
        "    original_token = data.x[mask_idx, 0].clone()\n",
        "    masked_data = data.clone()\n",
        "    masked_data.x[mask_idx] = mask_token_id\n",
        "\n",
        "    return masked_data, original_token, mask_idx"
      ],
      "metadata": {
        "id": "F5eBtTtwR4pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_splits(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "\n",
        "    total_size = len(dataset)\n",
        "    indices = list(range(total_size))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    train_size = int(train_ratio * total_size)\n",
        "    val_size = int(val_ratio * total_size)\n",
        "\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:train_size + val_size]\n",
        "    test_indices = indices[train_size + val_size:]\n",
        "\n",
        "    train_dataset = [dataset[i] for i in train_indices]\n",
        "    val_dataset = [dataset[i] for i in val_indices]\n",
        "    test_dataset = [dataset[i] for i in test_indices]\n",
        "\n",
        "    print(f\"set sizes: - train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n"
      ],
      "metadata": {
        "id": "ba8grEaNhOfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(dataset_name='ogbg-code2', sample_ratio=1):\n",
        "    print(\"loading dataset\")\n",
        "    pyg_dataset = PygGraphPropPredDataset(name=dataset_name)\n",
        "    split_index = pyg_dataset.get_idx_split()\n",
        "\n",
        "    train_indices = split_index['train']\n",
        "    train_subset_indices = random.sample(train_indices.tolist(), max(1, int(len(train_indices) * sample_ratio)))\n",
        "    dataset_subset = [pyg_dataset[i] for i in train_subset_indices]\n",
        "    print(f\"Number of graphs in subset: {len(dataset_subset)}\")\n",
        "\n",
        "    print(\"Applying masking to nodes\")\n",
        "    masked_data = []\n",
        "    labels = []\n",
        "    mask_indices = []\n",
        "\n",
        "    for data in dataset_subset:\n",
        "        masked_data_item, original_token, mask_idx = mask_node_token(data)\n",
        "        if mask_idx < data.num_nodes:\n",
        "            masked_data.append(masked_data_item)\n",
        "            labels.append(int(original_token.item()))\n",
        "            mask_indices.append(mask_idx)\n",
        "\n",
        "    print(\"Creating dataset\")\n",
        "    masked_dataset = MaskedDataset(masked_data, labels, mask_indices)\n",
        "\n",
        "    print(\"Creating splits\")\n",
        "    train_data, val_data, test_data = create_data_splits(masked_dataset)\n",
        "\n",
        "    batch_size = 128\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    vocab_size = get_max_vocab_size(train_loader)\n",
        "    print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "    return {\n",
        "        'train_loader': train_loader,\n",
        "        'val_loader': val_loader,\n",
        "        'test_loader': test_loader,\n",
        "        'vocab_size': vocab_size\n",
        "    }"
      ],
      "metadata": {
        "id": "OL5cIxSdhNWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, dropout_rate: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        self.query = nn.Linear(in_channels, hidden_channels)\n",
        "        self.key = nn.Linear(in_channels, hidden_channels)\n",
        "        self.value = nn.Linear(in_channels, hidden_channels)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(in_channels)\n",
        "        self.norm2 = nn.LayerNorm(hidden_channels)\n",
        "\n",
        "        self.gru = nn.GRUCell(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.output_proj = nn.Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.norm1(x)\n",
        "        identity = x\n",
        "\n",
        "        src, dst = edge_index\n",
        "\n",
        "        q = self.query(x[dst])\n",
        "        k = self.key(x[src])\n",
        "        v = self.value(x[src])\n",
        "\n",
        "        scores = torch.sum(q * k, dim=-1) / torch.sqrt(torch.tensor(self.hidden_channels).float())\n",
        "\n",
        "        attention_weights = scatter_softmax(scores, dst, dim=0)\n",
        "\n",
        "        weighted_messages = v * attention_weights.unsqueeze(-1)\n",
        "\n",
        "        messages = scatter_add(weighted_messages, dst, dim=0, dim_size=x.size(0))\n",
        "\n",
        "        h = self.gru(x, messages)\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = self.output_proj(h)\n",
        "        h = self.dropout_layer(h)\n",
        "\n",
        "        return h + identity\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int,\n",
        "                 num_layers: int = 2, dropout_rate: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embed = nn.Linear(in_channels, hidden_channels)\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_channels)\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            GATLayer(hidden_channels, hidden_channels, dropout_rate=dropout_rate) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_norm = nn.LayerNorm(hidden_channels)\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(hidden_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_channels, out_channels)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.embed(x.float())\n",
        "        x = self.norm(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout_layer(x)\n",
        "\n",
        "        initial_x = x\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "\n",
        "        x = x + initial_x\n",
        "\n",
        "        x = self.output_norm(x)\n",
        "        return self.output(x)"
      ],
      "metadata": {
        "id": "F909dBvdgDVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_vocab_size(train_loader):\n",
        "    max_target = 0\n",
        "    for batch in train_loader:\n",
        "        max_target = max(max_target, batch.y.max().item())\n",
        "    return max_target + 1"
      ],
      "metadata": {
        "id": "k5Aze72Rcxvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(11)\n",
        "torch.manual_seed(11)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(11)\n",
        "\n",
        "processed_data = preprocess_dataset()"
      ],
      "metadata": {
        "id": "tKjmKw5xkSti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def evaluate_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    class_correct = defaultdict(int)\n",
        "    class_total = defaultdict(int)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            outputs = model(batch)\n",
        "\n",
        "            predictions = []\n",
        "            start_idx = 0\n",
        "            for i in range(batch.num_graphs):\n",
        "                num_nodes = int(torch.sum((batch.batch == i).int()))\n",
        "                mask_idx = min(int(batch.mask_index[i]), num_nodes - 1)\n",
        "                node_idx = start_idx + mask_idx\n",
        "                predictions.append(outputs[node_idx])\n",
        "                start_idx += num_nodes\n",
        "\n",
        "            predictions = torch.stack(predictions)\n",
        "            loss = criterion(predictions, batch.y).item()\n",
        "            total_loss += loss\n",
        "\n",
        "            preds = predictions.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "            for pred, true in zip(preds.cpu(), batch.y.cpu()):\n",
        "                class_total[true.item()] += 1\n",
        "                if pred == true:\n",
        "                    class_correct[true.item()] += 1\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    unique_classes = np.unique(all_labels)\n",
        "\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro', labels=unique_classes)\n",
        "    f1_per_class = f1_score(all_labels, all_preds, average=None, labels=unique_classes)\n",
        "\n",
        "    f1_dict = dict(zip(unique_classes, f1_per_class))\n",
        "\n",
        "    total = sum(class_total.values())\n",
        "    correct = sum(class_correct.values())\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    class_metrics = {}\n",
        "    for class_idx in class_total.keys():\n",
        "        class_metrics[class_idx] = {\n",
        "            'accuracy': class_correct[class_idx] / class_total[class_idx],\n",
        "            'correct': class_correct[class_idx],\n",
        "            'total': class_total[class_idx],\n",
        "            'f1': f1_dict.get(class_idx, 0.0)\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'class_metrics': class_metrics\n",
        "    }\n",
        "\n",
        "def train(model, train_loader, val_loader, test_loader, num_epochs, device, learning_rate=1e-3):\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_f1 = 0.0\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        batch_correct = 0\n",
        "        batch_total = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(batch)\n",
        "\n",
        "            predictions = []\n",
        "            start_idx = 0\n",
        "            for i in range(batch.num_graphs):\n",
        "                num_nodes = int(torch.sum((batch.batch == i).int()))\n",
        "                mask_idx = min(int(batch.mask_index[i]), num_nodes - 1)\n",
        "                node_idx = start_idx + mask_idx\n",
        "                predictions.append(outputs[node_idx])\n",
        "                start_idx += num_nodes\n",
        "\n",
        "            predictions = torch.stack(predictions)\n",
        "            loss = criterion(predictions, batch.y)\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            preds = predictions.argmax(dim=1)\n",
        "            batch_correct += (preds == batch.y).sum().item()\n",
        "            batch_total += len(batch.y)\n",
        "\n",
        "            if (batch_idx + 1) % 5 == 0:\n",
        "                current_loss = epoch_loss / (batch_idx + 1)\n",
        "                current_acc = batch_correct / batch_total\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss = {current_loss:.4f}\")\n",
        "\n",
        "        val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step(val_metrics['f1_macro'])\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "        print(f\"Train Accuracy: {batch_correct/batch_total:.4f}\")\n",
        "        print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
        "        print(f\"Val F1 (macro): {val_metrics['f1_macro']:.4f}\")\n",
        "\n",
        "        if val_metrics['f1_macro'] > best_val_f1:\n",
        "            best_val_f1 = val_metrics['f1_macro']\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            print(\"new best model\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"stop early\"\")\n",
        "                break\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    test_metrics = evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"\\nFinal Test Results:\")\n",
        "    print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "    print(f\"Test F1 (macro): {test_metrics['f1_macro']:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-class Test Metrics:\")\n",
        "    for class_idx, metrics in test_metrics['class_metrics'].items():\n",
        "        print(f\"Class {class_idx}:\")\n",
        "        print(f\"  Accuracy: {metrics['accuracy']:.4f} ({metrics['correct']}/{metrics['total']})\")\n",
        "        print(f\"  F1-score: {metrics['f1']:.4f}\")\n",
        "\n",
        "    return model, test_metrics"
      ],
      "metadata": {
        "id": "mXDwfplvVHqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def setup_save_directory():\n",
        "    save_dir = '/content/model_results'\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    return save_dir\n",
        "\n",
        "def save_run_metrics(model_name, run_id, metrics, save_dir):\n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'run_id': run_id,\n",
        "        'accuracy': metrics['accuracy'],\n",
        "        'f1_macro': metrics['f1_macro'],\n",
        "        'class_metrics': metrics['class_metrics']\n",
        "    }\n",
        "\n",
        "    filename = os.path.join(save_dir, f\"metrics_{model_name}_{run_id}.json\")\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f)\n",
        "    print(f\"Saved metrics to: {filename}\")\n",
        "\n",
        "def run_multiple_trainings(model_class, model_params, train_params, num_runs=3):\n",
        "    save_dir = setup_save_directory()\n",
        "    all_metrics = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(f\"\\nStarting Run {run + 1}/{num_runs}\")\n",
        "\n",
        "        model = model_class(**model_params).to('cuda')\n",
        "\n",
        "        trained_model, metrics = train(model=model, **train_params)\n",
        "        save_run_metrics('gat_ablation', run, metrics, save_dir)\n",
        "\n",
        "        all_metrics.append(metrics)\n",
        "\n",
        "        print(f\"Run {run + 1} completed\")\n",
        "        print(f\"F1 Macro: {metrics['f1_macro']:.4f}\")\n",
        "\n",
        "    print(f\"\\nAll results saved in: {save_dir}\")\n",
        "    return all_metrics\n",
        "\n",
        "model_params = {\n",
        "    'in_channels': 2,\n",
        "    'hidden_channels': 64,\n",
        "    'out_channels': processed_data['vocab_size'],\n",
        "    'num_layers': 2,\n",
        "    'dropout_rate': 0.1\n",
        "}\n",
        "\n",
        "train_params = {\n",
        "    'train_loader': processed_data['train_loader'],\n",
        "    'val_loader': processed_data['val_loader'],\n",
        "    'test_loader': processed_data['test_loader'],\n",
        "    'num_epochs': 3,\n",
        "    'device': 'cuda',\n",
        "    'learning_rate': 1e-3\n",
        "}\n",
        "\n",
        "metrics_list = run_multiple_trainings(\n",
        "    model_class=GAT,\n",
        "    model_params=model_params,\n",
        "    train_params=train_params,\n",
        "    num_runs=3\n",
        ")"
      ],
      "metadata": {
        "id": "3F0yQmmpq_i4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}